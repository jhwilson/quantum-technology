# Introduction

In 2019 @AruteMartinis2019, Google reached a significant milestone in quantum computing when they achieved "Quantum Advantage"--demonstrating for the first time that a quantum computer could surpass the capabilities of classical computers, albeit for a specific, specialized task.
Using their Sycamore processor with 53 qubits, they showed that sampling from a quantum circuit a million times took only 200 seconds, while the equivalent task would take approximately 10,000 years on a classical supercomputer.
This breakthrough marked a turning point in quantum computing, igniting increased interest and investment in quantum technologies. 
Building on this success, in December 2024, Google announced their new Willow chip @GoogleQuantumAIandCollaboratorsZobrist2024, representing their latest advancement in quantum hardware.
Today, numerous companies and research institutions worldwide are racing to develop quantum hardware, each pursuing different technological approaches to challenge the computational limits of classical computers.

::: {.callout-note}
## What is Quantum Advantage?
Quantum advantage (or quantum supremacy) refers to the demonstration that a quantum computer can solve a specific problem significantly faster than the best known classical algorithm. However, the problem doesn't necessarily need to be useful--it just needs to be well-defined and verifiable.
:::

This text aims to give undergraduate students a comprehensive introduction to quantum technology and computation. 
We will begin by exploring the fundamental mathematical framework that underlies quantum computing, building from basic principles to more advanced concepts.
Crucially, we will see what specific things a quantum computer can achieve that a classical computer would struggle with.
With this foundation, we will examine three of the most promising current quantum computing technologies: superconducting qubits, photonic quantum computing, and ion traps.
Each of these approaches offers unique advantages and faces distinct challenges, which we will analyze in detail.

To bridge theory with practice, we will utilize IBM's Qiskit software platform to implement basic quantum computations, providing hands-on experience with quantum programming.
As we progress, we will explore critical practical considerations in quantum computing, including error mitigation and correction strategies.
Time permitting, we will venture into the cutting-edge field of topological quantum computing, which offers a potentially more robust approach to quantum computation.

Throughout this text, we will maintain a balanced perspective, examining both the tremendous potential and significant challenges facing quantum computing technology.
Our goal is to equip students with both theoretical understanding and practical insights into this rapidly evolving field.

## The Quantum-Classical Arms Race

The story of Google's quantum supremacy claim illustrates a fascinating dynamic in the field of quantum computing--an ongoing arms race between quantum and classical algorithms. When Google first announced their achievement with the Sycamore processor @AruteMartinis2019, they estimated that their quantum sampling task would take a classical supercomputer approximately 10,000 years. However, within months, IBM researchers developed improved classical algorithms that could potentially perform the same calculation in just 2.5 days @PednaultWisnieff2019.
Further work even demonstrated that using tensor networks, the problem could be solved _faster_ on a modern superconductor with ExaFLOPS performance @PanZhang2022.

This back-and-forth highlights several important lessons. First, it demonstrates the remarkable adaptability of classical computing. As quantum computers advance, classical algorithm developers find increasingly clever ways to simulate quantum systems or solve specific problems more efficiently. This competition drives innovation in both fields--quantum hardware must continually improve to maintain its advantage, while classical algorithms become more sophisticated in response.

Second, it serves as a cautionary tale about interpreting quantum computing announcements, particularly those aimed at the general public. While the achievement of quantum advantage represents a genuine milestone, the initial 10,000-year estimate proved overly optimistic. This pattern has repeated with various quantum computing companies, where marketing claims sometimes outpace peer-reviewed scientific validation. For students and researchers in the field, it's crucial to maintain a balanced perspective--acknowledging genuine breakthroughs while critically evaluating bold claims.

The recent announcement of Google's Willow chip @GoogleQuantumAIandCollaboratorsZobrist2024 represents another step forward, but should be viewed within this context of ongoing competition and careful validation. This healthy tension between quantum and classical approaches ultimately benefits both fields, pushing the boundaries of what's computationally possible while maintaining rigorous scientific standards.


## Early Computing: The Lesson of Transistors {#sec-transistors}

In Ref. @NielsenChuang2011 there are a few quotes about early computing

> _"Computers in the future may weigh no more than 1.5 tons."_
> –Popular Mechanics, forecasting the relentless march of science, 1949

> _"I think there is a world market for maybe five computers."_
> –Thomas Watson, chairman of IBM, 1943

These quotes raise important points about early technology. While the theory of modern computing really took off with Turing in 1937 @Turing1937, the technological advancement necessary for modern computurs would not occur until later: when the transistor came about.

### The Dream: Field Effect Transistors 

::: {#fig-MOSFETs layout-ncol=2}

![P-channel](images/P-Channel%20IGFET%20Diagram.svg){width=50% #fig-P-MOSFETenh}

![N-channel](images/N-Channel%20IGFET%20Diagram.svg){width=50% #fig-N-MOSFETenh}

Example Circuit diagrams for MOSFETs (_enh_)
:::

To enable classical computing, we need something like a "switch" that can be on and off, keeping track of whether or not something like, current is flowing. This is hard to do with traditional circuit elements like resistors, capacitors, and inductors.
It requires something more nonlinear: A switch that only allows current flow when a voltage is applied, a **transistor**.

@fig-MOSFETs shows two of the types of circuit diagrams for a bipolar junction transistor (G = gate, S = source, and D = drain)[^1]. A voltage applied at the gate enables a larger current to flow between collector and emitter. There are also _bipolar junction transistors_ that use a smaller current to enable a larger current, in this case there is "base", "collector", and "emitter". 


With these building blocks, logical gates can be created, but *how* to build these? Which device can be made small and in abundance? And what challenges were encountered on the way. 

::: {.callout-tip}
## Scale of Modern Computing in your pocket
The iPhone A17 Pro chip's 19 billion transistors would cover an area of about 1 square centimeter. If each transistor were the size of a grain of rice, they would cover an area larger than 75 football fields! This incredible miniaturization is what enables modern computing.
::: 

It was recognized early on that semiconductors provided an ideal platform for these kinds of circuits, and much work was done to try and create the above "field effect transistors." Schematically, these take the form:

![Schematic of Field Effect Transistor[^2]](images/FET%20Cross%20Section.svg){width=50% #fig-FET-schematic}

In @fig-FET-schematic, electrons flow from source to drain, but only when the "gate" has an applied voltage to it (effectively "lowering the barrier" for electrons to get through). This theory was sound and based on the recently developed quantum electron theory of metals developed by Wolfgang Pauli, Werner Heisenberg, Arnold Sommerfeld, Felix Bloch, and Rudolf Peierls @Hoddeson1981. And indeed, the people at Bell labs worked on this problem theoretically and experimentally for the beginning in the 1930s (for a full history, see @Hoddeson1981). Despite the strong foundations though, creating a field effect transistor turned out to be difficult, and in the process Brattain and Bardeen instead created the point-contact transistor.

### The Point Contact Transistor

::: {#fig-point-contact-transistor layout-ncol=2}

![Schematic of the point contact transistor[^3].](images/Point%20Contact%20Transistor.svg){#fig-PCT-schematic width=80%}

![Replica of first transistor](images/First%20Transistor%20Replica.jpg){#fig-PCT-replica}

The point contact transistor.
:::

The point-contact transistor, invented in 1947, worked quite differently from the field-effect design. Instead of using a voltage at a gate to control current flow, it used two very closely spaced metal contacts pressed against a semiconductor (typically germanium). One contact, called the emitter, would inject positive charge carriers (holes) into the semiconductor. The second contact, called the collector, would collect these carriers - but crucially, the amount of current that could flow through the collector could be controlled by small changes in the emitter current[^4]. A schematic and image of a replica of the original device are illustrated in @fig-point-contact-transistor.

This amplification effect, where a small current controls a larger one, was revolutionary--though the exact physics behind it wasn't fully understood at the time. The key was that the metal contacts created special regions in the semiconductor where the positive carriers modified the barrier for current flow from the bulk material. While the detailed quantum mechanics is complex, you can think of it like creating "paths" that electrons prefer to take through the material, with the emitter current controlling how easily electrons can flow along these paths to the collector.

While point-contact transistors were eventually superseded by more reliable and easier-to-manufacture designs, they represented a crucial breakthrough in electronics. They proved that solid-state devices could indeed amplify electrical signals.
However, they were quite large. The original design for a field effect transistor would be needed, and they key resided in understanding and control the _surface physics_ of semiconductors.

### Surface physics and transistors

The key challenge in creating field effect transistors lay in understanding and controlling the surface properties of semiconductors. To understand why this was so difficult, let's break it down:

When a semiconductor crystal (like silicon) ends at a surface, something interesting happens. The regular pattern of atoms is suddenly interrupted - imagine a neat stack of blocks suddenly ending in mid-air. This interruption creates what we call "surface states" - special energy levels that electrons can occupy right at the surface of the material.

These surface states turned out to be extremely problematic for making transistors. Remember that in a field effect transistor, we want to control the flow of electrons using an electric field from the gate (see @fig-FET-schematic). However, these surface states acted like tiny electron traps, capturing and holding onto electrons. When electrons got stuck in these states, they effectively "screened" or blocked the electric field from the gate, preventing it from controlling the current flow through the semiconductor.

This screening effect was so strong that early attempts at field effect transistors simply didn't work--no matter how strong a voltage was applied to the gate, it couldn't effectively control the current flow. It was like trying to control a water flow with a valve, but having something constantly blocking the valve from moving.

The breakthrough came in the 1950s when researchers, particularly at Bell Labs, realized they needed to chemically "passivate" the semiconductor surface--essentially finding ways to neutralize these problematic surface states. The key discovery was that growing a thin layer of silicon dioxide (SiO₂) on silicon created a much more stable interface with far fewer problematic surface states. This oxide layer also served as an excellent insulator between the gate and the semiconductor.

This seemingly simple solution--growing an oxide layer--was actually a remarkable achievement that required precise control of material chemistry and manufacturing processes. It finally allowed the creation of practical field effect transistors, leading to the modern MOSFET (Metal-Oxide-Semiconductor Field Effect Transistor) that forms the backbone of today's electronics.

The success of this approach also highlights an important lesson in technology development: sometimes the biggest breakthroughs come not from changing the fundamental design, but from finding ways to control and manage the subtle physical effects that prevent a good design from working in practice.

### Where are we with quantum computing?

Imagine that this course existed back in the 1950s and we called it "Computing Technology." Transistors were still coming online to enable computation at scale and we had both the information science and material theory to achieve it:

1. We knew what was needed to do universal classical computation.
2. We had the quantum theory of metals to describe how to build components (transistors) to achieve classical computation.

However, the engineering challenges took decades to resolve. It was only when we resolved those that computation as we currently envision it took off and we could have classical computers.

For quantum computing, we are in a similar situation:

1. We know what is needed to do universal quantum computation.
2. We have the quantum theory of photons, atoms, and superconductivity to achieve quantum computation.

However, as we will see in what follows, we have significant engineering challenges to achieve these in practice. While we will be largely concerned with the physics that make #2 possible in this course (and we'll touch on #1 for the first part of the course), we will pay attention to the strengths and weaknesses in the physics that lead to more pressing engineering challenges.

::: {.callout-note}
## Historical Parallel
Just as the theory of classical computation @Turing1937 preceded practical computers by decades, we now have the theory of quantum computation @NielsenChuang2011 but face significant engineering challenges. The key difference is that we're trying to control individual quantum systems rather than classical electrical currents.
:::

## Review of the Postulates of Quantum Mechanics

Quantum mechanics is built out of some basic postulates which are crucial to understand for quantum computation. When we talk about a "system" in these postulates, we will be thinking of two-level systems which we can label 0 or 1 (for our qubit). However, we will state them generally since many applications require some basic work to reduce the complicated system down to just the qubits we are interested in.

::: {.callout-important}
## Mathematical Notation Guide
Throughout this text, we'll use:

- $\ket{\psi}$ ("ket psi"): Represents a quantum state
- $\bra{\phi}$ ("bra phi"): The dual vector to $\ket{\phi}$
- $\braket{\phi|\psi}$: The inner product between states
- $\otimes$: The tensor product operation

These notations provide a compact way to describe quantum systems.
:::

### Postulate I: The Hilbert space

> A state in an isolated physical system $S$ can be described by a set of _normalized state vectors_--$\ket{\psi}$ and all vectors related by a phase $\ket{\psi'} = e^{i\phi}\ket{\psi}$--belonging in the Hilbert space $\mathcal H_S$. 

importantly, a Hilbert space is equipped with an inner product (much like the dot product in three-dimensions) $\braket{\alpha | \beta}$.

We also need rules for attaching Hilbert spaces to one another. Afterall, we will need to use more than one qubit to do anything interesting.

> The Hilbert space of a composite system is the _tensor product_ of the two individual systems $\mathcal H_{AB} = \mathcal H_A \otimes \mathcal H_B$.

### Postulate II: Physical Observables and Measurements {#sec-postulate-observable}

This postulate is also sometimes called the **Born rule**.

In order to measure the system (e.g., "where is the particle?" or "Is the qubit a 0 or 1?"), we need to know what _physical observables_ are 

> (i) Every physical observable $a$ can be described as a Hermitian operator $A$ acting in the Hilbert Space. 

Formally, a Hermitian operator has $A = A^\dagger$ where $A^\dagger$ is the _conjugate transpose_ of $A$. These operators have a whole set of orthonormal eigenstates $A\ket{a_n} = a_n\ket{a_n}$ for a _real_ number $a_n$ (orthonormal means $\braket{a_n| a_m} = \delta_{nm}$). These mathematical details are important for how we will perform measurements

> (ii) When a physical observable with operator $A$ is measured on a normalized eigenstate $\ket{\psi}$, the result is an eigenvalue $a_n$ of that operator with probability $p_n = \lvert \braket{a_n |\psi} \rvert^2$ (or in the case of a degeneracy $d$, $p_n = \sum_{i=1}^d \lvert \braket{a_n, i | \psi}\vert^2$).

If we measure $A$ repeatedly, we are naturally lead to the expectation value $\braket{\psi | A |\psi} = a = \sum_n a_n p_n$, the average result of repeated quantum mechanical measurements.
Once a measurement is performed, however, the state is changed, for this we need an operator $P_n$ which projects onto the eigenspace of $A$ associated with the eigenvalue $a_n$.

> (iii) When a measurement of the observable $A$ gives a results $a_n$, the state is changed to be the _normalized projection_ of $\ket{\psi}$ to the eigenspace associated with $a_n$ 
> $$ \ket{\psi} \quad \implies \quad \frac{P_n \ket{\psi}}{\sqrt{\braket{\psi|P_n | \psi}}} $$

If the eigenstates are not degenerate, then $\ket{\psi} \implies \ket{a_n}$. However, we will find that we will often have degenerate states, and in that case
$$
\ket{\psi} \implies \frac{\sum_{i=1}^d \braket{a_n, i | \psi} \ket{a_n, i}}{\sqrt{\sum_{i=1}^d \lvert \braket{a_n,i | \psi}\rvert^2}}.
$$
One comfortable with the braket notation might notice that within this, $P_n = \sum_{i=1}^d \ket{a_n,i}\bra{a_n,i}$.

### Postulate III: Time-evolution of a system {#sec-postulate-unitary}

While we will talk about Hamiltonians, often in quantum computation we have gates that do not require these. In this case, we state this postulate as abstractly as possible

> The time evolution  of a closed system from some time $t_0$ and state $\ket{\psi_0}$ to a final time $t$ and state $\ket{\psi}$ can be described as a unitary transformation 
> $$ \ket{\psi} = U(t,t_0) \ket{\psi_0}.$$

This postulate is necessary for us to maintain probabilities. Unitary operators have the property that $UU^\dagger = U^\dagger U = \mathbb{1}$, and so 
$$1 = \braket{\psi_0|\psi_0} = \braket{\psi_0|U^\dagger U| \psi_0} = \braket{\psi|\psi}.$$

In the case of time-independent _Hamiltonian dynamics,_ the operator takes the form $U = e^{-i H t/\hbar}$ for a hermitian energy operator $H$ called the _Hamiltonian_.

## A Brief History of Computing: From Classical to Quantum

The concept that information is physical underlies both classical and quantum computation. Even in the earliest systems of record-keeping, we see physical objects encoding information:

-	Kish Tablet (3500 BCE): A limestone tablet from Kish showing a record of pictographic writing.
-	Quipu (2600–1900 BCE): A system of knotted ropes used by the Inca civilization for keeping records. The color, order, and number of knots all represented quantifiable or categorical data.

These examples highlight that from the very beginning, the act of storing and manipulating information has always had a physical basis—though the underlying physics often remained implicit for centuries.

### Classical Computing Foundations

The paradigm of classical computing rests on a few fundamental ideas:

-	**Boolean Logic & Universal Gates**: All classical computers can be built from a finite set of universal logical gates (e.g., \{NAND\}, \{NOR\}, or \{AND, NOT\}). 

Most Boolean operations (AND, OR, NAND, etc.) are inherently irreversible: once a bit is erased or overwritten, the original state cannot be recovered from the output alone.

-	**Extended (Physical) Church–Turing Thesis**: A probabilistic Turing machine can efficiently simulate any realistic physical model of computation with at most polynomial overhead. 

This thesis, while unproven, underpins the expectation that classical computers (or at least classical models) are sufficient for simulating any physical system in principle.
Quantum computation potentially challenges this with a polynomial in time algorithm (Shor's algorithm) that is exponential in time classically.

### The Emergence of Quantum Information

While classical computing relies on bits that are strictly 0 or 1, quantum computing introduces powerful new concepts:

- **Superposition**: A quantum bit (qubit) can be in a linear combination of basis states (e.g., simultaneously “0” and “1” with certain complex amplitudes).
- **Entanglement**: Two or more qubits can become correlated in such a way that measuring one affects the outcomes for the others, even across vast distances.

The key question that launched the field of quantum information was whether these uniquely quantum properties—superposition and entanglement—could be exploited to perform computations more efficiently than any classical device.

In his seminal work, Simulating Physics with Computers @Feynman1982, Richard Feynman observed that simulating quantum many-body systems on classical computers seems to require exponential resources. He posed the idea of harnessing genuine quantum systems themselves for simulation, planting the seeds for quantum computation as a research field.

Then, in a series of groundbreaking results between the 1980s and 1990s, researchers demonstrated that quantum computers could, in principle, outperform classical computers for certain tasks:

1.	Deutsch (1985) @Deutsch1985: Showed that it is possible to carry out a simple computational task on a quantum computer faster than any classical algorithm.
2.	Deutsch–Jozsa (1992) @DeutschJozsa1992: Introduced a deterministic quantum algorithm that is exponentially faster (in the worst case) than any deterministic classical algorithm.
3.	Bernstein–Vazirani (1992) @BernsteinVazirani1997: Demonstrated a probabilistic quantum algorithm faster than any probabilistic classical algorithm.
4.	Simon (1994) @Simon1997: Provided a probabilistic quantum algorithm that is exponentially faster than any probabilistic classical algorithm for a specific promise problem.
5.	Shor (1994) @Shor1994: Showed how to factor integers efficiently, providing an exponential speedup over the best known classical methods. This result was particularly striking for cryptography, as factoring large numbers underpins many encryption schemes.

Alongside these algorithmic milestones, researchers began to delineate the limitations: not every problem can be exponentially sped up by quantum methods. For instance, Grover’s algorithm (1996) @Grover1996 for unstructured search yields a quadratic speedup (from $N$ to $\sqrt{N}$)—still better than classical, but not the exponential leap that Shor’s algorithm provides for factoring.

### Analog vs. Digital Quantum Simulation

As the field grew, quantum simulation branched into two approaches:

- Analog Quantum Simulation: Uses a controllable quantum system to mimic a target quantum system. The interactions in the simulator closely resemble the interactions in the system of interest.
- Digital Quantum Simulation: Decomposes a quantum evolution into a sequence of discrete gates (a “universal” set of quantum gates), akin to how classical digital computers function using logical gate operations.

Both approaches aim to exploit quantum mechanics to tackle problems in mathematics, physics, chemistry, and materials science that remain intractable for classical supercomputers.

Amid ongoing research, the interplay between classical and quantum paradigms remains a vibrant area of exploration. While classical computing infrastructure continues to be indispensable, quantum computing offers the promise of qualitatively new capabilities—provided we can tame the noise, errors, and fragilities inherent to quantum states.

## Quantum Technologies

One major difference between the hindsight-history we have for classical computation and the current state of quantum computers is that are many platforms vying to enable quantum computation. There are arguments for and against each platform, and even arguments for using a combination of platforms. Here we give a list of some of the technologies and highlight the ones we will be surveying in this course.

**Platforms covered in this course**

- **Superconducting qubits**: Artificial atoms made from superconducting circuits that operate at ultra-low temperatures. Currently the most mature platform, used by companies like IBM and Google.

- **Trapped ions**: Individual atoms held in place by electromagnetic fields. Known for having very long coherence times and high-fidelity gates. Major players include IonQ and Quantinuum (formerly of Honeywell).

- **Photonic quantum computers**: Use particles of light (photons) as qubits. Can operate at room temperature and naturally interface with quantum communication systems. Being developed by companies like PsiQuantum and Xanadu.

**Other platforms we will not cover**

- **Silicon quantum dots**: Quantum bits made from individual electrons trapped in silicon, similar to classical semiconductor technology. Could potentially leverage existing manufacturing processes.

- **Neutral atoms and Rydberg arrays**: Individual neutral atoms arranged in arrays using laser beams. When excited to high-energy Rydberg states, atoms can interact strongly with their neighbors. Can create large numbers of identical qubits with programmable interactions. Companies like QuEra are pursuing this approach.

- **NV centers**: Quantum bits made from nitrogen-vacancy defects in diamond. Can operate at room temperature and have long coherence times, making them particularly promising for quantum sensing and networking applications.

**A platform we will cover if time permits**

- **Topological qubits**: A theoretical approach that would use special quantum states of matter to create error-protected qubits. Still in early research stages but could offer significant advantages if realized.

::: {.callout-warning}
## Current State of Quantum Computing
As of 2024, the largest quantum computers have around 50-100 physical qubits optimistically, but these are noisy and require error correction. For comparison, your smartphone has billions of classical bits. This highlights the early stage of quantum computing development and the engineering challenges ahead.
:::

Each platform has its own advantages and challenges in terms of scalability, error rates, coherence times, and manufacturing complexity. The field is still evolving, and it's possible that different platforms may be optimal for different applications.


[^1]: Note that P or N stands for positive or negative carriers.
[^2]: Made by [VectorVoyager](https://commons.wikimedia.org/wiki/User:VectorVoyager) and licensed under Creative Commons [Attribution-Share Alike 3.0 Unported](https://creativecommons.org/licenses/by-sa/3.0/deed.en) 
[^3]: Licensed under Creative Commons [CC0 1.0 Universal Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/deed.en).
[^4]: For details on how this was made, see [How the first transistor worked](https://spectrum.ieee.org/transistor-history)