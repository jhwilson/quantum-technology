[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantum Technology",
    "section": "",
    "text": "Preface\nThe goal for this text is to be a snapshot in time of quantum technologies: the good, the bad, and the ugly. As of 2024, there has been a large amount of industry interest and progress to develop quantum technologies, and a student approaching this industry should have the basic knowledge to understand what is trying to be achieved and how they are trying to achieve it. To cut through the PR of industry, this text offers the author’s personal perspective on what has been achieved, where things need to go, and the challenges to get there. This is not meant to be an authoritative guide on any one of the technologies presented here, but a jumping off point for the interested student.\nFinally, this is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 The Quantum-Classical Arms Race\nIn 2019  [1], Google reached a significant milestone in quantum computing when they achieved “Quantum Advantage”–demonstrating for the first time that a quantum computer could surpass the capabilities of classical computers, albeit for a specific, specialized task. Using their Sycamore processor with 53 qubits, they showed that sampling from a quantum circuit a million times took only 200 seconds, while the equivalent task would take approximately 10,000 years on a classical supercomputer. This breakthrough marked a turning point in quantum computing, igniting increased interest and investment in quantum technologies. Building on this success, in December 2024, Google announced their new Willow chip  [2], representing their latest advancement in quantum hardware. Today, numerous companies and research institutions worldwide are racing to develop quantum hardware, each pursuing different technological approaches to challenge the computational limits of classical computers.\nThis text aims to give undergraduate students a comprehensive introduction to quantum technology and computation. We will begin by exploring the fundamental mathematical framework that underlies quantum computing, building from basic principles to more advanced concepts. Crucially, we will see what specific things a quantum computer can achieve that a classical computer would struggle with. With this foundation, we will examine three of the most promising current quantum computing technologies: superconducting qubits, photonic quantum computing, and ion traps. Each of these approaches offers unique advantages and faces distinct challenges, which we will analyze in detail.\nTo bridge theory with practice, we will utilize IBM’s Qiskit software platform to implement basic quantum computations, providing hands-on experience with quantum programming. As we progress, we will explore critical practical considerations in quantum computing, including error mitigation and correction strategies. Time permitting, we will venture into the cutting-edge field of topological quantum computing, which offers a potentially more robust approach to quantum computation.\nThroughout this text, we will maintain a balanced perspective, examining both the tremendous potential and significant challenges facing quantum computing technology. Our goal is to equip students with both theoretical understanding and practical insights into this rapidly evolving field.\nThe story of Google’s quantum supremacy claim illustrates a fascinating dynamic in the field of quantum computing–an ongoing arms race between quantum and classical algorithms. When Google first announced their achievement with the Sycamore processor  [1], they estimated that their quantum sampling task would take a classical supercomputer approximately 10,000 years. However, within months, IBM researchers developed improved classical algorithms that could potentially perform the same calculation in just 2.5 days  [3]. Further work even demonstrated that using tensor networks, the problem could be solved faster on a modern superconductor with ExaFLOPS performance  [4].\nThis back-and-forth highlights several important lessons. First, it demonstrates the remarkable adaptability of classical computing. As quantum computers advance, classical algorithm developers find increasingly clever ways to simulate quantum systems or solve specific problems more efficiently. This competition drives innovation in both fields–quantum hardware must continually improve to maintain its advantage, while classical algorithms become more sophisticated in response.\nSecond, it serves as a cautionary tale about interpreting quantum computing announcements, particularly those aimed at the general public. While the achievement of quantum advantage represents a genuine milestone, the initial 10,000-year estimate proved overly optimistic. This pattern has repeated with various quantum computing companies, where marketing claims sometimes outpace peer-reviewed scientific validation. For students and researchers in the field, it’s crucial to maintain a balanced perspective–acknowledging genuine breakthroughs while critically evaluating bold claims.\nThe recent announcement of Google’s Willow chip  [2] represents another step forward, but should be viewed within this context of ongoing competition and careful validation. This healthy tension between quantum and classical approaches ultimately benefits both fields, pushing the boundaries of what’s computationally possible while maintaining rigorous scientific standards.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-transistors",
    "href": "intro.html#sec-transistors",
    "title": "1  Introduction",
    "section": "1.2 Early Computing: The Lesson of Transistors",
    "text": "1.2 Early Computing: The Lesson of Transistors\nIn Ref.  [5] there are a few quotes about early computing\n\n“Computers in the future may weigh no more than 1.5 tons.” –Popular Mechanics, forecasting the relentless march of science, 1949\n\n\n“I think there is a world market for maybe five computers.” –Thomas Watson, chairman of IBM, 1943\n\nThese quotes raise important points about early technology. While the theory of modern computing really took off with Turing in 1937  [6], the technological advancement necessary for modern computurs would not occur until later: when the transistor came about.\n\n1.2.1 The Dream: Field Effect Transistors\n\n\n\n\n\n\n\n\n\n\n\n(a) P-channel\n\n\n\n\n\n\n\n\n\n\n\n(b) N-channel\n\n\n\n\n\n\n\nFig. 1.1: Example Circuit diagrams for MOSFETs (enh)\n\n\n\nTo enable classical computing, we need something like a “switch” that can be on and off, keeping track of whether or not something like, current is flowing. This is hard to do with traditional circuit elements like resistors, capacitors, and inductors. It requires something more nonlinear: A switch that only allows current flow when a voltage is applied, a transistor.\nFig. 1.1 shows two of the types of circuit diagrams for a bipolar junction transistor (G = gate, S = source, and D = drain)1. A voltage applied at the gate enables a larger current to flow between collector and emitter. There are also bipolar junction transistors that use a smaller current to enable a larger current, in this case there is “base”, “collector”, and “emitter”.\nWith these building blocks, logical gates can be created, but how to build these? Which device can be made small and in abundance? And what challenges were encountered on the way.\n\n\n\n\n\n\nScale of Modern Computing in your pocket\n\n\n\nThe iPhone A17 Pro chip’s 19 billion transistors would cover an area of about 1 square centimeter. If each transistor were the size of a grain of rice, they would cover an area larger than 75 football fields! This incredible miniaturization is what enables modern computing.\n\n\nIt was recognized early on that semiconductors provided an ideal platform for these kinds of circuits, and much work was done to try and create the above “field effect transistors.” Schematically, these take the form:\n\n\n\n\n\n\nFig. 1.2: Schematic of Field Effect Transistor2\n\n\n\nIn Fig. 1.2, electrons flow from source to drain, but only when the “gate” has an applied voltage to it (effectively “lowering the barrier” for electrons to get through). This theory was sound and based on the recently developed quantum electron theory of metals developed by Wolfgang Pauli, Werner Heisenberg, Arnold Sommerfeld, Felix Bloch, and Rudolf Peierls  [7]. And indeed, the people at Bell labs worked on this problem theoretically and experimentally for the beginning in the 1930s (for a full history, see  [7]). Despite the strong foundations though, creating a field effect transistor turned out to be difficult, and in the process Brattain and Schockley instead created the point-contact transistor.\n\n\n1.2.2 The Point Contact Transistor\n\n\n\n\n\n\n\n\n\n\n\n(a) Schematic of the point contact transistor3.\n\n\n\n\n\n\n\n\n\n\n\n(b) Replica of first transistor\n\n\n\n\n\n\n\nFig. 1.3: The point contact transistor.\n\n\n\nThe point-contact transistor, invented in 1947, worked quite differently from the field-effect design. Instead of using a voltage at a gate to control current flow, it used two very closely spaced metal contacts pressed against a semiconductor (typically germanium). One contact, called the emitter, would inject positive charge carriers (holes) into the semiconductor. The second contact, called the collector, would collect these carriers - but crucially, the amount of current that could flow through the collector could be controlled by small changes in the emitter current4. A schematic and image of a replica of the original device are illustrated in Fig. 1.3.\nThis amplification effect, where a small current controls a larger one, was revolutionary–though the exact physics behind it wasn’t fully understood at the time. The key was that the metal contacts created special regions in the semiconductor where the positive carriers modified the barrier for current flow from the bulk material. While the detailed quantum mechanics is complex, you can think of it like creating “paths” that electrons prefer to take through the material, with the emitter current controlling how easily electrons can flow along these paths to the collector.\nWhile point-contact transistors were eventually superseded by more reliable and easier-to-manufacture designs, they represented a crucial breakthrough in electronics. They proved that solid-state devices could indeed amplify electrical signals. However, they were quite large. The original design for a field effect transistor would be needed, and they key resided in understanding and control the surface physics of semiconductors.\n\n\n1.2.3 Surface physics and transistors\nThe key challenge in creating field effect transistors lay in understanding and controlling the surface properties of semiconductors. To understand why this was so difficult, let’s break it down:\nWhen a semiconductor crystal (like silicon) ends at a surface, something interesting happens. The regular pattern of atoms is suddenly interrupted - imagine a neat stack of blocks suddenly ending in mid-air. This interruption creates what we call “surface states” - special energy levels that electrons can occupy right at the surface of the material.\nThese surface states turned out to be extremely problematic for making transistors. Remember that in a field effect transistor, we want to control the flow of electrons using an electric field from the gate (see Fig. 1.2). However, these surface states acted like tiny electron traps, capturing and holding onto electrons. When electrons got stuck in these states, they effectively “screened” or blocked the electric field from the gate, preventing it from controlling the current flow through the semiconductor.\nThis screening effect was so strong that early attempts at field effect transistors simply didn’t work–no matter how strong a voltage was applied to the gate, it couldn’t effectively control the current flow. It was like trying to control a water flow with a valve, but having something constantly blocking the valve from moving.\nThe breakthrough came in the 1950s when researchers, particularly at Bell Labs, realized they needed to chemically “passivate” the semiconductor surface–essentially finding ways to neutralize these problematic surface states. The key discovery was that growing a thin layer of silicon dioxide (SiO₂) on silicon created a much more stable interface with far fewer problematic surface states. This oxide layer also served as an excellent insulator between the gate and the semiconductor.\nThis seemingly simple solution–growing an oxide layer–was actually a remarkable achievement that required precise control of material chemistry and manufacturing processes. It finally allowed the creation of practical field effect transistors, leading to the modern MOSFET (Metal-Oxide-Semiconductor Field Effect Transistor) that forms the backbone of today’s electronics.\nThe success of this approach also highlights an important lesson in technology development: sometimes the biggest breakthroughs come not from changing the fundamental design, but from finding ways to control and manage the subtle physical effects that prevent a good design from working in practice.\n\n\n1.2.4 Where are we with quantum computing?\nImagine that this course existed back in the 1950s and we called it “Computing Technology.” Transistors were still coming online to enable computation at scale and we had both the information science and material theory to achieve it:\n\nWe knew what was needed to do universal classical computation.\nWe had the quantum theory of metals to describe how to build components (transistors) to achieve classical computation.\n\nHowever, the engineering challenges took decades to resolve. It was only when we resolved those that computation as we currently envision it took off and we could have classical computers.\nFor quantum computing, we are in a similar situation:\n\nWe know what is needed to do universal quantum computation.\nWe have the quantum theory of photons, atoms, and superconductivity to achieve quantum computation.\n\nHowever, as we will see in what follows, we have significant engineering challenges to achieve these in practice. While we will be largely concerned with the physics that make #2 possible in this course (and we’ll touch on #1 for the first part of the course), we will pay attention to the strengths and weaknesses in the physics that lead to more pressing engineering challenges.\n\n\n\n\n\n\nHistorical Parallel\n\n\n\nJust as the theory of classical computation  [6] preceded practical computers by decades, we now have the theory of quantum computation  [5] but face significant engineering challenges. The key difference is that we’re trying to control individual quantum systems rather than classical electrical currents.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#review-of-the-postulates-of-quantum-mechanics",
    "href": "intro.html#review-of-the-postulates-of-quantum-mechanics",
    "title": "1  Introduction",
    "section": "1.3 Review of the Postulates of Quantum Mechanics",
    "text": "1.3 Review of the Postulates of Quantum Mechanics\nQuantum mechanics is built out of some basic postulates which are crucial to understand for quantum computation. When we talk about a “system” in these postulates, we will be thinking of two-level systems which we can label 0 or 1 (for our qubit). However, we will state them generally since many applications require some basic work to reduce the complicated system down to just the qubits we are interested in.\n\n\n\n\n\n\nMathematical Notation Guide\n\n\n\nThroughout this text, we’ll use:\n\n\\(\\ket{\\psi}\\) (“ket psi”): Represents a quantum state\n\\(\\bra{\\phi}\\) (“bra phi”): The dual vector to \\(\\ket{\\phi}\\)\n\\(\\braket{\\phi|\\psi}\\): The inner product between states\n\\(\\otimes\\): The tensor product operation\n\nThese notations provide a compact way to describe quantum systems.\n\n\n\n1.3.1 Postulate I: The Hilbert space\n\nA state in an isolated physical system \\(S\\) can be described by a set of normalized state vectors–\\(\\ket{\\psi}\\) and all vectors related by a phase \\(\\ket{\\psi'} = e^{i\\phi}\\ket{\\psi}\\)–belonging in the Hilbert space \\(\\mathcal H_S\\).\n\nimportantly, a Hilbert space is equipped with an inner product (much like the dot product in three-dimensions) \\(\\braket{\\alpha | \\beta}\\).\nWe also need rules for attaching Hilbert spaces to one another. Afterall, we will need to use more than one qubit to do anything interesting.\n\nThe Hilbert space of a composite system is the tensor product of the two individual systems \\(\\mathcal H_{AB} = \\mathcal H_A \\otimes \\mathcal H_B\\).\n\n\n\n1.3.2 Postulate II: Physical Observables and Measurements\nThis postulate is also sometimes called the Born rule.\nIn order to measure the system (e.g., “where is the particle?” or “Is the qubit a 0 or 1?”), we need to know what physical observables are\n\n\nEvery physical observable \\(a\\) can be described as a Hermitian operator \\(A\\) acting in the Hilbert Space.\n\n\nFormally, a Hermitian operator has \\(A = A^\\dagger\\) where \\(A^\\dagger\\) is the conjugate transpose of \\(A\\). These operators have a whole set of orthonormal eigenstates \\(A\\ket{a_n} = a_n\\ket{a_n}\\) for a real number \\(a_n\\) (orthonormal means \\(\\braket{a_n| a_m} = \\delta_{nm}\\)). These mathematical details are important for how we will perform measurements\n\n\nWhen a physical observable with operator \\(A\\) is measured on a normalized eigenstate \\(\\ket{\\psi}\\), the result is an eigenvalue \\(a_n\\) of that operator with probability \\(p_n = \\lvert \\braket{a_n |\\psi} \\rvert^2\\) (or in the case of a degeneracy \\(d\\), \\(p_n = \\sum_{i=1}^d \\lvert \\braket{a_n, i | \\psi}\\vert^2\\)).\n\n\nIf we measure \\(A\\) repeatedly, we are naturally lead to the expectation value \\(\\braket{\\psi | A |\\psi} = a = \\sum_n a_n p_n\\), the average result of repeated quantum mechanical measurements. Once a measurement is performed, however, the state is changed, for this we need an operator \\(P_n\\) which projects onto the eigenspace of \\(A\\) associated with the eigenvalue \\(a_n\\).\n\n\nWhen a measurement of the observable \\(A\\) gives a results \\(a_n\\), the state is changed to be the normalized projection of \\(\\ket{\\psi}\\) to the eigenspace associated with \\(a_n\\) \\[ \\ket{\\psi} \\quad \\implies \\quad \\frac{P_n \\ket{\\psi}}{\\sqrt{\\braket{\\psi|P_n | \\psi}}} \\]\n\n\nIf the eigenstates are not degenerate, then \\(\\ket{\\psi} \\implies \\ket{a_n}\\). However, we will find that we will often have degenerate states, and in that case \\[\n\\ket{\\psi} \\implies \\frac{\\sum_{i=1}^d \\braket{a_n, i | \\psi} \\ket{a_n, i}}{\\sqrt{\\sum_{i=1}^d \\lvert \\braket{a_n,i | \\psi}\\rvert^2}}.\n\\] One comfortable with the braket notation might notice that within this, \\(P_n = \\sum_{i=1}^d \\ket{a_n,i}\\bra{a_n,i}\\).\n\n\n1.3.3 Postulate III: Time-evolution of a system\nWhile we will talk about Hamiltonians, often in quantum computation we have gates that do not require these. In this case, we state this postulate as abstractly as possible\n\nThe time evolution of a closed system from some time \\(t_0\\) and state \\(\\ket{\\psi_0}\\) to a final time \\(t\\) and state \\(\\ket{\\psi}\\) can be described as a unitary transformation \\[ \\ket{\\psi} = U(t,t_0) \\ket{\\psi_0}.\\]\n\nThis postulate is necessary for us to maintain probabilities. Unitary operators have the property that \\(UU^\\dagger = U^\\dagger U = \\mathbb{1}\\), and so \\[1 = \\braket{\\psi_0|\\psi_0} = \\braket{\\psi_0|U^\\dagger U| \\psi_0} = \\braket{\\psi|\\psi}.\\]\nIn the case of time-independent Hamiltonian dynamics, the operator takes the form \\(U = e^{-i H t/\\hbar}\\) for a hermitian energy operator \\(H\\) called the Hamiltonian.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#a-brief-history-of-computing-from-classical-to-quantum",
    "href": "intro.html#a-brief-history-of-computing-from-classical-to-quantum",
    "title": "1  Introduction",
    "section": "1.4 A Brief History of Computing: From Classical to Quantum",
    "text": "1.4 A Brief History of Computing: From Classical to Quantum\nThe concept that information is physical underlies both classical and quantum computation. Even in the earliest systems of record-keeping, we see physical objects encoding information:\n\nKish Tablet (3500 BCE): A limestone tablet from Kish showing a record of pictographic writing.\nQuipu (2600–1900 BCE): A system of knotted ropes used by the Inca civilization for keeping records. The color, order, and number of knots all represented quantifiable or categorical data.\n\nThese examples highlight that from the very beginning, the act of storing and manipulating information has always had a physical basis—though the underlying physics often remained implicit for centuries.\n\n1.4.1 Classical Computing Foundations\nThe paradigm of classical computing rests on a few fundamental ideas:\n\nBoolean Logic & Universal Gates: All classical computers can be built from a finite set of universal logical gates (e.g., {NAND}, {NOR}, or {AND, NOT}).\n\nMost Boolean operations (AND, OR, NAND, etc.) are inherently irreversible: once a bit is erased or overwritten, the original state cannot be recovered from the output alone.\n\nExtended (Physical) Church–Turing Thesis: A probabilistic Turing machine can efficiently simulate any realistic physical model of computation with at most polynomial overhead.\n\nThis thesis, while unproven, underpins the expectation that classical computers (or at least classical models) are sufficient for simulating any physical system in principle. Quantum computation potentially challenges this with a polynomial in time algorithm (Shor’s algorithm) that is exponential in time classically.\n\n\n1.4.2 The Emergence of Quantum Information\nWhile classical computing relies on bits that are strictly 0 or 1, quantum computing introduces powerful new concepts:\n\nSuperposition: A quantum bit (qubit) can be in a linear combination of basis states (e.g., simultaneously “0” and “1” with certain complex amplitudes).\nEntanglement: Two or more qubits can become correlated in such a way that measuring one affects the outcomes for the others, even across vast distances.\n\nThe key question that launched the field of quantum information was whether these uniquely quantum properties—superposition and entanglement—could be exploited to perform computations more efficiently than any classical device.\nIn his seminal work, Simulating Physics with Computers  [8], Richard Feynman observed that simulating quantum many-body systems on classical computers seems to require exponential resources. He posed the idea of harnessing genuine quantum systems themselves for simulation, planting the seeds for quantum computation as a research field.\nThen, in a series of groundbreaking results between the 1980s and 1990s, researchers demonstrated that quantum computers could, in principle, outperform classical computers for certain tasks:\n\nDeutsch (1985)  [9]: Showed that it is possible to carry out a simple computational task on a quantum computer faster than any classical algorithm.\nDeutsch–Jozsa (1992)  [10]: Introduced a deterministic quantum algorithm that is exponentially faster (in the worst case) than any deterministic classical algorithm.\nBernstein–Vazirani (1992)  [11]: Demonstrated a probabilistic quantum algorithm faster than any probabilistic classical algorithm.\nSimon (1994)  [12]: Provided a probabilistic quantum algorithm that is exponentially faster than any probabilistic classical algorithm for a specific promise problem.\nShor (1994)  [13]: Showed how to factor integers efficiently, providing an exponential speedup over the best known classical methods. This result was particularly striking for cryptography, as factoring large numbers underpins many encryption schemes.\n\nAlongside these algorithmic milestones, researchers began to delineate the limitations: not every problem can be exponentially sped up by quantum methods. For instance, Grover’s algorithm (1996)  [14] for unstructured search yields a quadratic speedup (from \\(N\\) to \\(\\sqrt{N}\\))—still better than classical, but not the exponential leap that Shor’s algorithm provides for factoring.\n\n\n1.4.3 Analog vs. Digital Quantum Simulation\nAs the field grew, quantum simulation branched into two approaches:\n\nAnalog Quantum Simulation: Uses a controllable quantum system to mimic a target quantum system. The interactions in the simulator closely resemble the interactions in the system of interest.\nDigital Quantum Simulation: Decomposes a quantum evolution into a sequence of discrete gates (a “universal” set of quantum gates), akin to how classical digital computers function using logical gate operations.\n\nBoth approaches aim to exploit quantum mechanics to tackle problems in mathematics, physics, chemistry, and materials science that remain intractable for classical supercomputers.\nAmid ongoing research, the interplay between classical and quantum paradigms remains a vibrant area of exploration. While classical computing infrastructure continues to be indispensable, quantum computing offers the promise of qualitatively new capabilities—provided we can tame the noise, errors, and fragilities inherent to quantum states.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#quantum-technologies",
    "href": "intro.html#quantum-technologies",
    "title": "1  Introduction",
    "section": "1.5 Quantum Technologies",
    "text": "1.5 Quantum Technologies\nOne major difference between the hindsight-history we have for classical computation and the current state of quantum computers is that are many platforms vying to enable quantum computation. There are arguments for and against each platform, and even arguments for using a combination of platforms. Here we give a list of some of the technologies and highlight the ones we will be surveying in this course.\nPlatforms covered in this course\n\nSuperconducting qubits: Artificial atoms made from superconducting circuits that operate at ultra-low temperatures. Currently the most mature platform, used by companies like IBM and Google.\nTrapped ions: Individual atoms held in place by electromagnetic fields. Known for having very long coherence times and high-fidelity gates. Major players include IonQ and Quantinuum (formerly of Honeywell).\nPhotonic quantum computers: Use particles of light (photons) as qubits. Can operate at room temperature and naturally interface with quantum communication systems. Being developed by companies like PsiQuantum and Xanadu.\n\nOther platforms we will not cover\n\nSilicon quantum dots: Quantum bits made from individual electrons trapped in silicon, similar to classical semiconductor technology. Could potentially leverage existing manufacturing processes.\nNeutral atoms and Rydberg arrays: Individual neutral atoms arranged in arrays using laser beams. When excited to high-energy Rydberg states, atoms can interact strongly with their neighbors. Can create large numbers of identical qubits with programmable interactions. Companies like QuEra are pursuing this approach.\nNV centers: Quantum bits made from nitrogen-vacancy defects in diamond. Can operate at room temperature and have long coherence times, making them particularly promising for quantum sensing and networking applications.\n\nA platform we will cover if time permits\n\nTopological qubits: A theoretical approach that would use special quantum states of matter to create error-protected qubits. Still in early research stages but could offer significant advantages if realized.\n\n\n\n\n\n\n\nCurrent State of Quantum Computing\n\n\n\nAs of 2024, the largest quantum computers have around 50-100 physical qubits optimistically, but these are noisy and require error correction. For comparison, your smartphone has billions of classical bits. This highlights the early stage of quantum computing development and the engineering challenges ahead.\n\n\nEach platform has its own advantages and challenges in terms of scalability, error rates, coherence times, and manufacturing complexity. The field is still evolving, and it’s possible that different platforms may be optimal for different applications.\n\n\n\n\n[1] F. Arute et al., Quantum supremacy using a programmable superconducting processor, Nature 574, 505 (2019).\n\n\n[2] Google Quantum AI and Collaborators et al., Quantum error correction below the surface code threshold, Nature (2024).\n\n\n[3] E. Pednault, J. A. Gunnels, G. Nannicini, L. Horesh, and R. Wisnieff, Leveraging Secondary Storage to Simulate Deep 54-qubit Sycamore Circuits, arXiv:1910.09534 (2019).\n\n\n[4] F. Pan, K. Chen, and P. Zhang, Solving the Sampling Problem of the Sycamore Quantum Circuits, Physical Review Letters 129, 090502 (2022).\n\n\n[5] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information: 10th Anniversary Edition, Anniversary edition (Cambridge University Press, Cambridge ; New York, 2011).\n\n\n[6] A. M. Turing, On Computable Numbers, with an Application to the Entscheidungsproblem, Proceedings of the London Mathematical Society s2-42, 230 (1937).\n\n\n[7] L. Hoddeson, The Discovery of the Point-Contact Transistor, Historical Studies in the Physical Sciences 12, 41 (1981).\n\n\n[8] R. P. Feynman, Simulating physics with computers, International Journal of Theoretical Physics 21, 467 (1982).\n\n\n[9] D. Deutsch, Quantum theory, the Church–Turing principle and the universal quantum computer, Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences 400, 97 (1985).\n\n\n[10] D. Deutsch and R. Jozsa, Rapid solution of problems by quantum computation, Proceedings of the Royal Society of London. Series A: Mathematical and Physical Sciences 439, 553 (1992).\n\n\n[11] E. Bernstein and U. Vazirani, Quantum Complexity Theory, SIAM Journal on Computing 26, 1411 (1997).\n\n\n[12] D. R. Simon, On the Power of Quantum Computation, SIAM Journal on Computing 26, 1474 (1997).\n\n\n[13] P. W. Shor, Algorithms for Quantum Computation: Discrete Logarithms and Factoring, in Proceedings 35th Annual Symposium on Foundations of Computer Science (IEEE Comput. Soc. Press, Santa Fe, NM, USA, 1994), pp. 124–134.\n\n\n[14] L. K. Grover, A Fast Quantum Mechanical Algorithm for Database Search, in Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing - STOC ’96 (ACM Press, Philadelphia, Pennsylvania, United States, 1996), pp. 212–219.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Note that P or N stands for positive or negative carriers.↩︎\nMade by VectorVoyager and licensed under Creative Commons Attribution-Share Alike 3.0 Unported↩︎\nLicensed under Creative Commons CC0 1.0 Universal Public Domain Dedication.↩︎\nFor details on how this was made, see How the first transistor worked↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "qubit.html",
    "href": "qubit.html",
    "title": "2  The Qubit",
    "section": "",
    "text": "2.1 The Qubit Hilbert space\nThe fundamental building block of the classical computer was the bit: A 0 or 1 that could be manipulated by a classical computer (via transistors, see Section 1.2). In a similar manner, quantum computation has the “quantum bit” or just qubit, for short. This leads us to the linear algebra of \\(2\\times 2\\) matrices, as we will see. Despite the apparent simplicity, we can already see many of the key features of quantum mechanics in this simple system.\nA qubit will be in two-dimensional complex vector space equipped with an inner product.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Qubit</span>"
    ]
  },
  {
    "objectID": "qubit.html#the-qubit-hilbert-space",
    "href": "qubit.html#the-qubit-hilbert-space",
    "title": "2  The Qubit",
    "section": "",
    "text": "2.1.1 Qubit states\nFor the qubit, we associate two states with two different basis vectors: \\(\\ket{0}\\) and \\(\\ket{1}\\). This will be called the computational basis. The magic1 of quantum mechanics is that a state need not be just one or the other, but could be any linear superposition of these \\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1}.\n\\tag{2.1}\\] In this, we have adopted the bra-ket notation due to Dirac. While it can be quite useful, we can write this in terms of matrices and vectors \\[\n\\ket{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad \\ket{1} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n\\] This makes it clear that these states are orthogonal \\(\\braket{0|1} = 0\\).\nIn this case we have \\[\n\\ket{\\psi} = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}.\n\\] We also need the conjugate transpose, the Hermitian conjugate, of this vector, which will be a row-vector \\[\n\\bra{\\psi} = \\begin{bmatrix} \\alpha^* & \\beta^* \\end{bmatrix}.\n\\] The key feature of quantum mechanics is that these states must be normalized, meaning that the probability of finding the system in any state must sum to 1. This means that \\[\n\\braket{\\psi|\\psi} = \\begin{bmatrix} \\alpha^* & \\beta^* \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} = |\\alpha|^2 + |\\beta|^2 = 1.\n\\] In matrix notation, this is just the dot product of a vector with its complex conjugate.\n\n\n\n\n\n\nWhy Normalization Matters\n\n\n\nThe normalization condition \\(|\\alpha|^2 + |\\beta|^2 = 1\\) isn’t just mathematical convenience - it ensures probabilities add up to 100%! For example:\n\n\\(\\ket{\\psi} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\) gives 50-50 chance of measuring 0 or 1\n\\(\\ket{\\psi} = \\frac{\\sqrt{3}}{2}\\ket{0} + \\frac{1}{2}\\ket{1}\\) gives 75% chance of 0 and 25% chance of 1\n\n\n\nWe can also write operators that act on these states. The simplest operator is the Pauli \\(Z\\) operator, which in matrix form is \\[\nZ = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n\\] When this operator acts on our basis states, we find \\[\nZ\\ket{0} = \\ket{0}, \\quad Z\\ket{1} = -\\ket{1}.\n\\] This means that \\(\\ket{0}\\) and \\(\\ket{1}\\) are eigenstates of \\(Z\\) with eigenvalues \\(+1\\) and \\(-1\\) respectively. For a general state \\(\\ket{\\psi}\\), measuring \\(Z\\) will yield either \\(+1\\) or \\(-1\\), with probabilities determined by \\(|\\alpha|^2\\) and \\(|\\beta|^2\\) respectively.\n\n\n\n\n\n\nExample: Measuring a superposition state\n\n\n\nConsider the state \\(\\ket{\\psi} = \\frac35\\ket{0} + \\frac45\\ket{1}\\). When we measure this state in the \\(Z\\) basis:\nNotice that this state is normalized since \\((\\frac{3}{5})^2 + (\\frac{4}{5})^2 = 1\\). If we measure this state in the computational basis:\n\nWe’ll get outcome \\(\\ket{0}\\) with probability \\(|\\frac{3}{5}|^2 = 0.36\\) (36%)\nWe’ll get outcome \\(\\ket{1}\\) with probability \\(|\\frac{4}{5}|^2 = 0.64\\) (64%)\n\nAfter measurement, the state will collapse to either \\(\\ket{0}\\) or \\(\\ket{1}\\) with the above probabilities\n\n\n\n\n\n\n\n\nMeasurement Collapse in Practice\n\n\n\nWhen we say a quantum state “collapses” upon measurement, what actually happens in the lab?\n\nFor a superconducting qubit: We measure a voltage or current\nFor an ion trap: We detect scattered photons\nFor a photonic qubit: We count photons with a detector\n\nEach technology has its own way of converting quantum information into classical signals!\n\n\n\n\n2.1.2 Qubit operators\nSince this is linear algebra, we can write a general operator \\(\\mathcal O\\) as a matrix \\[\n\\mathcal O = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\\] Often, we are interested in the eigenvalues and eigenstates of these operators \\(\\mathcal O \\ket{\\psi_{i}} = \\lambda_i \\ket{\\psi_i}\\). Generically, we can find these by solving a polynomial equation \\[\n\\det(\\mathcal O - \\lambda I) = 0.\n\\]\nSolving this step-by-step \\[\n\\det(\\mathcal O - \\lambda I) = \\begin{vmatrix} a-\\lambda & b \\\\ c & d-\\lambda \\end{vmatrix} = 0,\n\\] which gives us \\[\n(a-\\lambda)(d-\\lambda) - bc = 0.\n\\] This is a quadratic equation that we can solve: \\[\n\\lambda^2 - (a+d)\\lambda + (ad-bc) = 0.\n\\]\nThe eigenvalues are therefore \\[\n\\lambda_{\\pm} = \\frac{a+d \\pm \\sqrt{(a-d)^2 + 4bc}}{2}.\n\\tag{2.2}\\]\nFor quantum mechanical observables (see Section 1.3.2), we are particularly interested in Hermitian operators where \\(\\mathcal O = \\mathcal O^\\dagger\\), \\[\n\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} a^* & c^* \\\\ b^* & d^* \\end{bmatrix}\n\\] This means that \\(a\\) and \\(d\\) must be real and \\(c = b^*\\). In this case, the eigenvalues are always real, as we can see from the Eq. 2.2.\nA particularly important class of operators are unitary operators, where \\(U^\\dagger U = UU^\\dagger = I\\). These are what we use for time-evolution, see Section 1.3.3.\nThese operators preserve the inner product between states: \\[\n\\braket{U\\psi|U\\phi} = \\braket{\\psi|\\phi}\n\\] For a \\(2\\times 2\\) matrix \\[\nU = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix},\n\\] the unitarity condition means that \\[\n\\begin{bmatrix} a^* & c^* \\\\ b^* & d^* \\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n\\]\nThis gives us several conditions:\n\n\\(|a|^2 + |c|^2 = 1\\) (normalization of first column)\n\\(|b|^2 + |d|^2 = 1\\) (normalization of second column)\n\\(ab^* + cd^* = 0\\) (orthogonality of columns)\n\nThis immediately gives us some insight into these operators. If we define, \\[\n\\ket{\\psi_1} = \\begin{bmatrix} a \\\\ c \\end{bmatrix}, \\quad \\ket{\\psi_2} = \\begin{bmatrix} b \\\\ d \\end{bmatrix},\n\\] then we have \\(\\braket{\\psi_1|\\psi_1} = 1 = \\braket{\\psi_2|\\psi_2}\\) and \\(\\braket{\\psi_1|\\psi_2} = 0\\).\nAn important property of unitary operators is that their eigenvalues always have magnitude 1, meaning they can be written as \\(e^{i\\theta}\\) for some real \\(\\theta\\). This makes them natural operators for describing quantum evolution.\n\n\n\n\n\n\nWhy Unitary?\n\n\n\nUnitary operators are special because they:\n\nPreserve the normalization of quantum states\nAre reversible (have an inverse)\nRepresent physical operations that conserve probability\n\nThis is why quantum gates must be unitary - they represent real physical processes that can be undone!\n\n\n\n\n2.1.3 The Pauli operators\nA particularly important set of operators are the Pauli operators. We’ve already seen the Pauli \\(Z\\) operator. The other two are2 \\[\nX = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}, \\quad Y = \\begin{bmatrix} 0 & -i \\\\ i & 0 \\end{bmatrix}.\n\\] These operators satisfy some important algebraic relations: \\[\nX^2 = Y^2 = Z^2 = I, \\quad XY = iZ, \\quad YZ = iX, \\quad ZX = iY.\n\\] We can additionally start to see some logical operations begin to appear; \\(X\\) operates on the computational basis as a NOT gate \\[\nX \\ket{0} = \\ket{1}, \\quad X \\ket{1} = \\ket{0}.\n\\]\n\n\n\n\n\n\nPauli Operators in Action\n\n\n\nThe Pauli operators represent quantum operations:\n\n\\(X\\) is like the classical NOT gate: flips between \\(\\ket{0}\\) and \\(\\ket{1}\\)\n\\(Z\\) adds a phase: leaves \\(\\ket{0}\\) alone but negates \\(\\ket{1}\\)\n\\(Y = iXZ\\) combines both operations.\n\nThese simple operations are building blocks for more complex quantum algorithms!\n\n\n\n\n\n\n\n\nExample: Applying operators\n\n\n\nLet’s apply the X (NOT) gate to our state \\(\\ket{\\psi} = (\\tfrac{3}{5}\\ket{0} + \\tfrac{4}{5}\\ket{1})\\): \\[\n\\begin{aligned}\nX\\ket{\\psi} &= X(\\tfrac{3}{5}\\ket{0} + \\tfrac{4}{5}\\ket{1}) \\\\\n&= \\tfrac{3}{5}X\\ket{0} + \\tfrac{4}{5}X\\ket{1} \\\\\n&= \\tfrac{3}{5}\\ket{1} + \\tfrac{4}{5}\\ket{0} \\\\\n&= \\tfrac{4}{5}\\ket{0} + \\tfrac{3}{5}\\ket{1}\n\\end{aligned}\n\\]\n\n\nThe full set of Pauli operators, along with the identity, form a complete basis for \\(2\\times 2\\) matrices, meaning we can write any operator as \\[\n\\mathcal O = aI + bX + cY + dZ,\n\\] where \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are complex numbers. We can extract each of these numbers, mathematically, with a trace operation \\[\n\\tr \\mathcal O = 2a, \\quad \\tr \\mathcal O X = 2b, \\quad \\tr \\mathcal O Y = 2c, \\quad \\tr \\mathcal O Z = 2d.\n\\] Note that separately, \\(X\\), \\(Y\\), and \\(Z\\) are Hermitian (and thus, observables). If \\(\\mathcal O\\) is an observable, then \\(\\mathcal O = \\mathcal O^\\dagger\\) immediately leads us to \\(a\\), \\(b\\), \\(c\\), and \\(d\\) being all real.\nWe can put constraints on these coefficients for unitary operators as well, and we leave this as an exercise for the reader.\nFinally, these operators have eigenstates as well, and we can define them as \\(X\\ket{\\pm} = \\pm \\ket{\\pm}\\) and \\(Y\\ket{\\pm i} = \\pm \\ket{\\pm i}\\), and they have the forms \\[\n\\begin{aligned}\n    \\ket{\\pm} & = \\tfrac1{\\sqrt2}(\\ket 0 \\pm \\ket 1), \\\\\n    \\ket{\\pm i} & = \\tfrac1{\\sqrt2}(\\ket 0 \\pm i \\ket 1).\n\\end{aligned}\n\\]\nWe will often want to change our basis from \\(\\ket{0}\\) and \\(\\ket{1}\\) to \\(\\ket{+}\\) and \\(\\ket{-}\\). This is accomplished with something called the Hadamard gate (we’ll call it \\(H\\), not to be confused with a Hamiltonian) and it is created specifically to change from computational basis to the \\(X\\) basis: \\(H \\ket{0} = \\ket{+}\\) and \\(H \\ket{1} = \\ket{-}\\). As a matrix it takes the form \\[\nH = \\frac1{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}.\n\\]\n\n\n\n\n\n\nThe Power of Hadamard\n\n\n\nThe Hadamard gate is one of the most important gates in quantum computing with useful properties:\n\nIt creates equal superpositions from computational basis states.\nIt’s its own inverse (\\(H^2 = I\\))\nIt’s used in nearly every quantum algorithm\nWhen applied to \\(n\\) qubits, it creates a superposition of all \\(2^n\\) possible bit strings!\n\n\n\n\n\n\n\n\n\nExample 3: The Hadamard Transform\n\n\n\nThe Hadamard gate is particularly important because it creates superposition states. Let’s see what happens when we apply it to \\(\\ket{0}\\): \\[\n\\begin{aligned}\nH\\ket{0} &= \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 & 1 \\\\ 1 & -1 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n&= \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\\\\n&= \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1}) \\\\\n&= \\ket{+}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Qubit</span>"
    ]
  },
  {
    "objectID": "qubit.html#sec-bloch-sphere",
    "href": "qubit.html#sec-bloch-sphere",
    "title": "2  The Qubit",
    "section": "2.2 The Bloch sphere",
    "text": "2.2 The Bloch sphere\nThe qubit itself is more than just a probability of being a 1 or a 0. A crucial bit of quantum information is the relative phase between the two states for instance \\[\n\\ket{\\psi} = \\tfrac1{\\sqrt2}( \\ket 0 + e^{i\\phi} \\ket 1),\n\\] and since all states are equivalent up to a total phase, we can write the amplitude of each state with a real number. In this case, if we set \\(\\ket{\\psi} = x \\ket{0} + y e^{i\\phi} \\ket 1\\), then we have \\(\\braket{\\psi|\\psi} = x^2 + y^2 = 1\\) for normalization. This is the equation for a circle, and writing out \\(x = \\cos(\\theta/2)\\) and \\(y = \\sin(\\theta/2)\\)3, we are lead to an angular representation of our state. \\[\n\\ket{\\psi} = \\cos(\\theta/2) \\ket{0} + \\sin(\\theta/2)e^{i\\phi} \\ket{1}.\n\\tag{2.3}\\] Let’s see how this state relates to our Pauli operators. If we calculate the expectation values of each operator:\n\\[\n\\begin{aligned}\n\\langle X \\rangle &= \\bra{\\psi}X\\ket{\\psi} \\\\\n&= (\\cos(\\theta/2) \\bra{0} + \\sin(\\theta/2)e^{-i\\phi} \\bra{1})\n   X\n   (\\cos(\\theta/2) \\ket{0} + \\sin(\\theta/2)e^{i\\phi} \\ket{1}) \\\\\n&= (\\cos(\\theta/2) \\bra{0} + \\sin(\\theta/2)e^{-i\\phi} \\bra{1})\n   (\\cos(\\theta/2) \\ket{1} + \\sin(\\theta/2)e^{i\\phi} \\ket{0}) \\\\\n&= \\sin(\\theta/2)\\cos(\\theta/2) e^{i\\phi} + \\cos(\\theta/2)\\sin(\\theta/2)e^{-i\\phi} \\\\\n&= \\sin(\\theta/2)\\cos(\\theta/2) (e^{i\\phi} + e^{-i\\phi}) \\\\\n&= 2\\sin(\\theta/2)\\cos(\\theta/2) \\cos\\phi \\\\\n&= \\sin \\theta \\cos\\phi.\n\\end{aligned}\n\\] We can carry out a similar calculation for \\(Y\\) and \\(Z\\) to obtain \\[\n\\begin{aligned}\n\\langle X \\rangle &= \\bra{\\psi}X\\ket{\\psi} = \\sin\\theta\\cos\\phi \\\\\n\\langle Y \\rangle &= \\bra{\\psi}Y\\ket{\\psi} = \\sin\\theta\\sin\\phi \\\\\n\\langle Z \\rangle &= \\bra{\\psi}Z\\ket{\\psi} = \\cos\\theta\n\\end{aligned}\n\\]\nThese expectation values give us coordinates , which are precisely the coordinates of a point on a unit sphere! This is why we call it the Bloch sphere. The angles \\(\\theta\\) and \\(\\phi\\) are the usual spherical coordinates.\n\n\n\n\n\n\nThe Bloch Sphere Geometry\n\n\n\n\nThe north pole (\\(\\theta=0\\)) corresponds to \\(\\ket{0}\\)\nThe south pole (\\(\\theta=\\pi\\)) corresponds to \\(\\ket{1}\\)\nThe equator (\\(\\theta=\\pi/2\\)) contains equal superposition of computational basis states:\n\n\\(\\phi=0\\) gives \\(\\ket{+}\\) (positive x-axis)\n\\(\\phi=\\pi\\) gives \\(\\ket{-}\\) (negative x-axis)\n\\(\\phi=\\pi/2\\) gives \\(\\ket{+i}\\) (positive y-axis)\n\\(\\phi=3\\pi/2\\) gives \\(\\ket{-i}\\) (negative y-axis)\n\n\n\n\n\n\n\n\n\n\nFig. 2.1: The Bloch sphere showing eigenstates of X, Y, and Z Pauli operators\n\n\n\n\n\n\n\n\n\nExample: Hadamard Gate on \\(\\ket{0}\\)\n\n\n\nThe Hadamard gate \\(H\\) takes the state \\(\\ket{0}\\) (north pole) to \\(\\ket{+}\\) (on the equator at \\(\\phi=0\\)). In terms of the Bloch sphere coordinates, this means:\n\nStarting point: \\(\\theta=0\\) (north pole)\nEnding point: \\(\\theta=\\pi/2\\), \\(\\phi=0\\) (positive x-axis)\n\nThe gate effectively rotates the state by 90° around the y-axis. Similarly, \\(H\\ket{1}\\) takes the south pole to \\(\\ket{-}\\) on the negative x-axis.\n\n\n\n2.2.1 General Unitary Rotations\nThe Hadamard example shows how unitary gates can rotate states on the Bloch sphere. More generally, any single-qubit unitary operation can be thought of as a rotation of the Bloch sphere. Let’s see how this works.\nA general rotation around a unit vector \\(\\vec{n} = (n_x, n_y, n_z)\\) by angle \\(\\theta\\) is given by \\[\nR_{\\vec{n}}(\\theta) = \\cos(\\theta/2)I - i\\sin(\\theta/2)(n_xX + n_yY + n_zZ).\n\\]\nFor example:\n\nRotation around z-axis: \\[ R_z(\\theta) = e^{-i\\theta Z/2} = \\begin{bmatrix} e^{-i\\theta/2} & 0 \\\\ 0 & e^{i\\theta/2} \\end{bmatrix} \\]\nRotation around x-axis: \\[ R_x(\\theta) = e^{-i\\theta X/2} = \\begin{bmatrix} \\cos(\\theta/2) & -i\\sin(\\theta/2) \\\\ -i\\sin(\\theta/2) & \\cos(\\theta/2) \\end{bmatrix} \\]\nRotation around y-axis: \\[R_y(\\theta) = e^{-i\\theta Y/2} = \\begin{bmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{bmatrix}\\]\n\nA remarkable fact is that any single-qubit unitary operation can be decomposed into three rotations around two different axes. This is known as the Euler angle decomposition: \\[\nU = e^{i\\alpha}R_z(\\phi)R_y(\\theta)R_z(\\psi)\n\\] where \\(\\alpha\\), \\(\\phi\\), \\(\\theta\\), and \\(\\psi\\) are real numbers. The global phase \\(e^{i\\alpha}\\) is often unimportant for quantum computing purposes.\n\n\n\n\n\n\nVisualizing Rotations\n\n\n\nThe Euler angle decomposition has a nice geometric interpretation:\n\nFirst rotation (\\(R_z(\\psi)\\)): Rotate around z-axis\nSecond rotation (\\(R_y(\\theta)\\)): Tilt to new latitude\nThird rotation (\\(R_z(\\phi)\\)): Rotate to final longitude\nGlobal phase (\\(e^{i\\alpha}\\)): Invisible in measurements\n\nThis is similar to how we specify points on Earth using latitude and longitude!\n\n\n\n\n2.2.2 The Phase Gate\nThe phase gate (often denoted as \\(S\\)) is another important single-qubit gate that adds a phase of \\(i\\) to the \\(\\ket{1}\\) state while leaving \\(\\ket{0}\\) unchanged:\n\\[\nS = \\begin{bmatrix} 1 & 0 \\\\ 0 & i \\end{bmatrix}\n\\]\nWhen acting on basis states: \\[\nS\\ket{0} = \\ket{0}, \\quad S\\ket{1} = i\\ket{1}\n\\]\nThe phase gate is equivalent to a \\(\\pi/2\\) rotation around the z-axis: \\(S = R_z(\\pi/2)\\). On the Bloch sphere, this corresponds to rotating a state by 90° around the z-axis.\n\n\n\n\n\n\nExample: Phase Gate on Superposition\n\n\n\nLet’s see what happens when we apply \\(S\\) to an equal superposition state:\n\\[\n\\begin{aligned}\nS\\ket{+} &= S\\left(\\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\right) \\\\\n&= \\frac{1}{\\sqrt{2}}(S\\ket{0} + S\\ket{1}) \\\\\n&= \\frac{1}{\\sqrt{2}}(\\ket{0} + i\\ket{1})\n\\end{aligned}\n\\]\nThis transforms \\(\\ket{+}\\) into \\(\\ket{+i}\\), rotating it from the positive x-axis to the positive y-axis on the Bloch sphere.\n\n\nThe phase gate is particularly important in quantum error correction and quantum algorithms where controlled phase operations are needed.\nMore generically, we can create any \\(R_z(\\phi)\\) gate to perform rotations about the \\(z\\)-axis (this is very useful for Shor’s algorithm). However, a common variant is the \\(T\\) gate, which is simply \\(R_z(\\pi/4)\\) and is one of the minimal components needed to achieve universal quantum computation.\n\n\n\n\n\n\nPhase gate leaves computational basis states alone\n\n\n\nNote that these gates only change superposition of computational basis states, so \\(S \\ket{0} = \\ket{0}\\) and \\(S\\ket{1} = i \\ket{1}\\). (Similarly for any gate made from rotations about the z-axis.)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Qubit</span>"
    ]
  },
  {
    "objectID": "qubit.html#quantum-circuits",
    "href": "qubit.html#quantum-circuits",
    "title": "2  The Qubit",
    "section": "2.3 Quantum Circuits",
    "text": "2.3 Quantum Circuits\nNow that we’ve covered the key single-qubit operations, we can start to think about how to represent sequences of these operations graphically using quantum circuits. In quantum circuits:\n\nQubits are represented as horizontal lines (called “wires”, see Fig. 2.2).\nGates are boxes or symbols placed on these wires (see Fig. 2.3 and Fig. 2.4).\nTime flows from left to right.\nMeasurements are represented by meters (see Fig. 2.5).\n\nOften, unless we are preparing a specific state for an algorithm, we may leave off the states from the ends of the “wire.” This wire, Fig. 2.2, you can think of as the identity.\n\n\n\n\n\n\n\n\nIdentity on \\(\\ket{0}\\)\n\n\n\n\n\n\n\nIdentity on \\(\\ket{1}\\)\n\n\n\n\n\n\n\nIdentity on \\(\\ket{\\psi}\\)\n\n\n\n\n\n\nFig. 2.2: Lines representing the evolution of a qubit (no gates applied)\n\n\n\nWhile in quantum state evolution we apply operators right-to-left, the two operations \\(X \\ket{0} = \\ket{1}\\) and \\(Z \\ket{+} = \\ket{-}\\) are represented in Fig. 2.3.\n\n\n\n\n\n\nImportant\n\n\n\nThe Pauli operators are both unitary and Hermitian, so they perform as quantum gates (which require unitary as Section 1.3.3 stipulates) and as observables (as Section 1.3.2 stipulates).\n\n\n\n\n\n\n\n\n\n\nApplication of the \\(X\\) gate.\n\n\n\n\n\n\n\nApplication of the \\(Z\\) gate.\n\n\n\n\n\n\nFig. 2.3: Examples of the Pauli operators as gates.\n\n\n\nThe Hadamard and phase gates can also be mixed in, Fig. 2.4, and we can even introduce the meter symbol for measurements, Fig. 2.5.\n\n\n\n\n\n\n\n\nApplication of the Hadamard gate\n\n\n\n\n\n\n\nApplication of the phase gate\n\n\n\n\n\n\nFig. 2.4: Examples of the Hadamard and phase gates.\n\n\n\n\n\n\n\n\n\n\n\nWill measure 1 with 100% probability\n\n\n\n\n\n\n\nWill measure 1 with 50% probability\n\n\n\n\n\n\nFig. 2.5: Examples of measurements. The meter is assumed to be measuring in the computational basis unless otherwise noted (i.e., measuring the \\(Z\\) operator).\n\n\n\nWhenever measurements are performed, we will often transform to the computational basis \\(\\ket{0}\\) and \\(\\ket{1}\\) to perform a measurement. By playing tricks with unitary transformations, we can measure other observables by mixing unitaries and measurements of the computational basis; dependent on the platform, this may or may not be necessary. In terms of diagrams, you should assume meters are measurements in the computational basis unless it is noted otherwise.\n\n\n\n\n\n\nReading Quantum Circuits\n\n\n\nQuantum circuits are read from left to right, just like reading text. Each horizontal line represents a qubit’s journey through time, and the boxes show what operations happen and when.\nThe measurement symbol at the end indicates when we extract classical information from our quantum system.\n\n\nThese diagrams give us a powerful visual language for describing quantum computations. Even complex algorithms can be broken down into sequences of these basic operations.\n\n\n\n\n\n\nExample: Gate Sequences\n\n\n\nConsider applying \\(H\\) then \\(Z\\) then \\(H\\) to \\(\\ket{0}\\):\n\n\n\nImplementing an \\(X\\) gate with \\(Z\\) and \\(H\\).\n\n\n\n\\(H\\ket{0} = \\ket{+}\\) (move to +x axis)\n\\(Z\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0} - \\ket{1})\\) (rotate around z by 90°)\n\\(H(\\frac{1}{\\sqrt{2}}(\\ket{0} - \\ket{1})) = \\ket{1}\\) (move to -z axis)\n\nThis sequence effectively implements the \\(X\\) gate!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Qubit</span>"
    ]
  },
  {
    "objectID": "qubit.html#noise-and-decoherence",
    "href": "qubit.html#noise-and-decoherence",
    "title": "2  The Qubit",
    "section": "2.4 Noise and decoherence",
    "text": "2.4 Noise and decoherence\nA large part of the course will consist of finding physical systems where we can identify certain states as \\(\\ket{0}\\) and \\(\\ket{1}\\), and then proceed to figure out how to perform the gates we need for quantum algorithms. In this way, this course could be called “two-level systems and where to find them.” However, these states are not pristine and so we need a way to talk about states that subject to noisy environments and loss of information.\nIn fact, environmental interactions can cause, amongst other issues:\n\nLoss of phase information (dephasing)\nLoss of energy (amplitude damping)\nRandom bit flips\n\nTo properly describe these noise processes, we need to move beyond pure state descriptions and introduce the density matrix formalism.\n\n2.4.1 The Density Matrix\nFor a pure quantum state \\(\\ket{\\psi}\\), the density matrix is defined as \\[\n\\rho = \\ket{\\psi}\\bra{\\psi}\n\\] For example, the computational basis states have density matrices: \\[\n\\ket{0}\\bra{0} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}, \\quad\n\\ket{1}\\bra{1} = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\]\nThe density matrix also provides a convenient way to calculate expectation values of observables. For any observable \\(A\\), the expectation value is given by: \\[\n\\langle A \\rangle = \\tr(A\\rho)\n\\] For a pure state \\(\\ket{\\psi}\\), this reduces to our familiar expression: \\[\n\\tr(A\\ket{\\psi}\\bra{\\psi}) = \\bra{\\psi}A\\ket{\\psi}\n\\]\n\n\n\n\n\n\nExample: Measuring \\(Z\\) with the Density Matrix\n\n\n\nConsider measuring \\(Z\\) for the state \\(\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\):\nThe density matrix is: \\[\n\\rho_+ = \\frac{1}{2}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n\\]\nUsing the formula for expecation values with \\(Z = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\): \\[\n\\langle Z \\rangle = \\tr(Z\\rho_+) = \\tr\\left(\\frac{1}{2}\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\\right) = 0\n\\]\nThis matches what we expect: \\(\\ket{+}\\) has equal probability of measuring \\(\\pm 1\\) for \\(Z\\).\n\n\nThe real power of density matrices comes from describing mixed states, which are statistical mixtures of pure states: \\[\n\\rho = \\sum_i p_i \\ket{\\psi_i}\\bra{\\psi_i}\n\\] where \\(p_i\\) are classical probabilities (\\(p_i \\geq 0\\), \\(\\sum_i p_i = 1\\)).\n\n\n\n\n\n\nProperties of Density Matrices\n\n\n\nAny valid density matrix must satisfy:\n\nHermiticity: \\(\\rho = \\rho^\\dagger\\)\nPositive semidefinite: \\(\\bra{\\phi}\\rho\\ket{\\phi} \\geq 0\\) for all \\(\\ket{\\phi}\\)\nUnit trace: \\(\\tr(\\rho) = 1\\)\nFor pure states: \\(\\tr(\\rho^2) = 1\\)\nFor mixed states: \\(\\tr(\\rho^2) &lt; 1\\)\n\n\n\n\nQuantum Operations with Density Matrices\nJust as we can evolve pure states with unitary operators, we can evolve density matrices. For a unitary operation \\(U\\), the density matrix transforms as: \\[\n\\rho \\rightarrow U\\rho U^\\dagger\n\\] This preserves all the properties of the density matrix we discussed above.\nWhen we make a measurement, we use projection operators \\(\\{P_m\\}\\) (like \\(\\ket{0}\\bra{0}\\) and \\(\\ket{1}\\bra{1}\\) for measuring in the computational basis). The probability of getting outcome \\(m\\) is: \\[\np(m) = \\tr(P_m \\rho)\n\\] After measuring and getting outcome \\(m\\), the state “collapses” to: \\[\n\\rho_m = \\frac{P_m \\rho P_m}{\\tr(P_m \\rho)}\n\\] where the denominator ensures the trace remains 1.\n\n\n\n\n\n\nQuantum Channels\n\n\n\nThe density matrix formalism really shines when describing general quantum evolution, including noise. Any physical process (unitary or not) can be described by a quantum channel \\(\\mathcal{E}\\): \\[\n\\rho \\rightarrow \\mathcal{E}(\\rho) = \\sum_k E_k \\rho E_k^\\dagger\n\\] The operators \\(E_k\\) (called Kraus operators) satisfy \\(\\sum_k E_k^\\dagger E_k = I\\) to preserve probability. This includes both ideal unitary evolution (one \\(E_k = U\\)) and noise processes (multiple \\(E_k\\)), as we’ll see in the next section.\n\n\n\n\nDensity Matrices and the Bloch Sphere\nThe density matrix formalism provides a beautiful geometric picture when combined with the Bloch sphere representation in Section 2.2. While pure states live on the surface of the Bloch sphere, mixed states live inside it. Any density matrix for a single qubit can be written as: \\[\n\\rho = \\frac{1}{2}(I + \\vec{r}\\cdot\\vec{\\sigma})\n\\] where \\(\\vec{r} = (r_x, r_y, r_z)\\) is called the Bloch vector and \\(\\vec{\\sigma} = (X,Y,Z)\\) are the Pauli matrices.\nThe length of \\(\\vec{r}\\) determines how mixed the state is:\n\nFor pure states, \\(|\\vec{r}| = 1\\) (surface of sphere)\nFor mixed states, \\(|\\vec{r}| &lt; 1\\) (inside sphere)\nFor the maximally mixed state, \\(\\vec{r} = 0\\) (center of sphere)\n\nThis gives us an intuitive picture of decoherence: noise processes like dephasing and bit flips move the Bloch vector inward from the surface, representing the loss of quantum information. The maximally mixed state at the center represents complete loss of information–equal probabilities for all measurement outcomes.\n\n\n\n\n\n\nConnection to Purity\n\n\n\nThe length of the Bloch vector is directly related to the purity \\(\\tr(\\rho^2)\\) we discussed earlier: \\[\n\\tr(\\rho^2) = \\frac{1 + |\\vec{r}|^2}{2}\n\\] This confirms that pure states (\\(|\\vec{r}| = 1\\)) have purity 1, while mixed states have purity less than 1.\n\n\n\n\n\n2.4.2 Modeling Noise\nA full discussion of different quantum channels can be found in  [3]. Here, we discuss some of the ones relevant for loss of quantum information.\nThe language of density matrices allows us to describe some of the errors that can occur in a precise manner\n\nRandom bit-flips\nIf there is probability \\(p\\) that a bit is flipped, we can encode this within a change of the density matrix: \\[\n\\rho \\rightarrow (1-p)\\rho + p X\\rho X,\n\\] where the first term describes the probability that the density matrix remains unchanged and the second describe the process of randomly flipping a bit.\n\n\nDephasing\nAs we’ve already mentioned the phase between \\(\\ket{0}\\) and \\(\\ket{1}\\) is useful information that we will take advantage of. However, some physical processes can scramble this phase in a way that could be inherently difficult to parse, in that case we can describe these processes with “dephasing” \\[\n\\rho \\rightarrow (1-p)\\rho + p Z \\rho Z.\n\\] This process has a clear action on \\(\\rho\\) when \\(p=1/2\\). To illustrate this, consider \\(\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1}\\), this has a probability of measuring \\(0\\) of \\(|\\alpha|^2\\) and a probabilty of measuring \\(1\\) of \\(|\\beta|^2\\), but there is also phase information between \\(\\alpha\\) and \\(\\beta\\) that tells us where on the Bloch sphere the state is located. Dephasing will eliminate this phase information.\nTo see this, the full density matrix before we apply dephasing is \\[\n\\rho = \\begin{bmatrix} |\\alpha|^2 & \\alpha\\beta^* \\\\ \\alpha^*\\beta & |\\beta|^2 \\end{bmatrix},\n\\] and after dephasing we will have the density matrix \\[\n\\frac12 \\rho + \\frac12 Z \\rho Z = \\begin{bmatrix} |\\alpha|^2 & 0 \\\\ 0 & |\\beta|^2 \\end{bmatrix}.\n\\] The off-diagonal components carried the relative phase information and it is now entirely lost, leaving us with a classical mixture of \\(\\ket{0}\\) and \\(\\ket{1}\\)\n\n\n\n\n\n\nMeasurement as Dephasing\n\n\n\nThis process of losing phase information is exactly what happens when we measure a quantum state in a particular basis! When we measure in the computational basis (\\(\\{\\ket{0}, \\ket{1}\\}\\)), we are effectively performing complete dephasing - we destroy all phase information between the basis states and are left with only the classical probabilities. This is why measurement is often described as “collapsing” the quantum state into classical information.\nThis connection between measurement and dephasing illustrates a fundamental aspect of quantum mechanics: the act of gaining classical information about a quantum system necessarily destroys some of its quantum properties, as formalized in the measurement postulates we discussed earlier.\n\n\n\n\n\n\n\n\nExample: Dephasing of a Superposition State\n\n\n\nConsider the state \\(\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\) under dephasing:\nInitial density matrix: \\[\n\\rho_0 = \\frac{1}{2}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n\\]\nAfter dephasing with probability \\(p\\): \\[\n\\rho(p) = \\frac{1}{2}\\begin{bmatrix} 1 & (1-2p) \\\\ (1-2p) & 1 \\end{bmatrix}\n\\]\nComplete dephasing (\\(p=\\frac{1}{2}\\)) yields the maximally mixed state: \\[\n\\rho(\\tfrac{1}{2}) = \\frac{1}{2}\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\]\n\n\n\n\nDepolarizing\nThe depolarizing channel represents one of the most severe forms of noise in quantum systems, often considered the “worst-case scenario” for quantum information. This channel transforms any input state into a mixture of itself and the maximally mixed state: \\[\n\\rho \\rightarrow (1-p)\\rho + p \\frac{I}{2},\n\\] where \\(p\\) represents the probability of depolarization. When \\(p=1\\), the state becomes completely mixed regardless of the input: \\[\n\\rho \\rightarrow \\frac{1}{2}I.\n\\]\nNote that this has eliminated any information about \\(\\ket{\\psi}\\)!\n\n\n\n\n\n\nExample: The Pauli Twirl\n\n\n\nAn interesting way to understand the depolarizing channel is through what’s called the “Pauli twirl”. The depolarizing channel can be written as a random application of Pauli operators: \\[\n\\rho \\rightarrow (1-p)\\rho + \\frac{p}{3}(X\\rho X + Y\\rho Y + Z\\rho Z).\n\\]\nThis means we can implement depolarizing noise by randomly applying X, Y, or Z gates with probability \\(p/3\\) each. This is particularly useful in quantum error correction, where we often want to simulate noise in a way that’s easy to analyze.\n\n\n\n\nAmplitude Damping\nIn many physical systems, one can lose information by having emission of energy. Physically, this could be an atom in some energy level and it emits a photon, falling to a lower energy level. One can imagine a process like this occuring with an operator \\[\nE_0 = \\sqrt{\\gamma} \\ket{0} \\bra{1},\n\\] which describes the process of taking an occupied state \\(\\ket{1}\\) and transforming it to \\(\\ket{0}\\) with probability \\(\\gamma\\). To fully encode this into a change in the density matrix, we need one other operator: the chance that nothing happens. A natural guess would be \\[\nE_1 = \\ket{0}\\bra{0} + \\sqrt{1-\\gamma} \\ket{1}\\bra{1},\n\\] which describes that there is unit probability of remaining \\(\\ket{0}\\) and probaiblity \\(1-\\gamma\\) of remaining in \\(\\ket{1}\\) if you start there.\nThe full operation on the density matrix is then \\[\n\\rho \\rightarrow E_1 \\rho E_1^\\dagger + E_0 \\rho E_0^\\dagger\n\\]\n\n\n\n\n\n\nExample: Amplitude Damping in Action\n\n\n\nLet’s see how amplitude damping affects a simple superposition state. Consider the initial state \\[\n\\ket{\\psi} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1}),\n\\] which has density matrix \\[\n\\rho = \\frac{1}{2}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}.\n\\]\nAfter applying amplitude damping with probability \\(\\gamma\\), the state becomes \\[\n\\begin{aligned}\n\\rho' &= E_1 \\rho E_1^\\dagger + E_0 \\rho E_0^\\dagger \\\\\n&= \\frac{1}{2}\\begin{bmatrix} 1+\\gamma & \\sqrt{1-\\gamma} \\\\ \\sqrt{1-\\gamma} & 1-\\gamma \\end{bmatrix}.\n\\end{aligned}\n\\]\nWe can see that:\n\nThe probability of being in \\(\\ket{1}\\) decreases by \\(\\gamma\\)\nThe probability of being in \\(\\ket{0}\\) increases by \\(\\gamma\\)\nThe off-diagonal coherence terms decay by \\(\\sqrt{1-\\gamma}\\)\n\nWhen \\(\\gamma = 1\\) (complete damping), the state becomes \\[\n\\rho' = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\ket{0}\\bra{0},\n\\] representing complete relaxation to the ground state.\n\n\n\n\nErasure Channel\nAnother important quantum channel is the erasure channel, which models the loss of a qubit to other accessible states. Unlike amplitude damping, where information leaks gradually, the erasure channel represents a complete loss of the qubit with probability \\(\\epsilon\\). When this happens, the qubit is replaced by an “error state” that we denote as \\(\\ket{e}\\). Importantly, \\(\\ket{e}\\) exists in a different part of the Hilbert space than our qubit states - it’s an additional state that flags that erasure has occurred.\n\n\n\n\n\n\nOrthogonality of Error State\n\n\n\nThe error state \\(\\ket{e}\\) is orthogonal to both \\(\\ket{0}\\) and \\(\\ket{1}\\), meaning \\(\\braket{e|0} = \\braket{e|1} = 0\\). This is crucial as it represents a state completely outside our original qubit space but still within the system itself.\n\n\nThe erasure can then be represented by \\[\n\\rho \\mapsto (1 - \\epsilon)\\rho + \\epsilon \\ket{e}\\bra{e}.\n\\]\nIn particular, this process\n\nPreserves the state with probability \\(1-\\epsilon\\)\nErases it completely with probability \\(\\epsilon\\), replacing it with the error state \\(\\ket{e}\\)\n\n\n\n\n\n\n\nExample: Erasure Channel in Action\n\n\n\nSince the erasure channel operates in an enlarged Hilbert space that includes the error state (in this case, making it 3-dimensional), let’s consider our superposition state embedded in this expanded space: \\[\n\\ket{\\psi} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1}) = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1}) = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n\\] After the erasure channel acts, the state becomes a mixed state: \\[\n\\rho' = (1-\\epsilon)\\begin{bmatrix}\n1/2 & 1/2 & 0 \\\\\n1/2 & 1/2 & 0 \\\\\n0 & 0 & 0\n\\end{bmatrix} + \\epsilon\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\frac{1-\\epsilon}{2} & \\frac{1-\\epsilon}{2} & 0 \\\\\n\\frac{1-\\epsilon}{2} & \\frac{1-\\epsilon}{2} & 0 \\\\\n0 & 0 & \\epsilon\n\\end{bmatrix}\n\\] This represents that with probability \\(1-\\epsilon\\) we still have our original state in the upper-left \\(2\\times2\\) block, but with probability \\(\\epsilon\\) we have completely lost it to the error state, represented by the 1 in the bottom-right corner.\n\n\nThe erasure channel is particularly important in quantum communication and error correction because the error state exists within the system Hilbert space itself (not the environment). This additional state in the system allows us to track and flag when errors occur–making it especially useful for quantum error correction protocols, unlike other noise channels where the errors are unknown and harder to detect.\n\n\n\n2.4.3 Physical Qubit Implementations\nThe quest for building practical quantum computers has led to several different approaches for implementing qubits. Each implementation aims to create a physical system that can reliably represent quantum states and allow for precise control and manipulation. We will explore in detail later what is needed for a good quantum device, but give a brief overview here of what the devices are.\n\nSuperconducting Qubits\n\nQubit are energy levels of charge or flux within a superconducting circuit.\nBased on superconducting circuits using Josephson junctions.\nOperate at extremely low temperatures (~20 mK).\nUsed by: IBM, Google, Rigetti.\n\n\n\nTrapped Ion Qubits\n\nQubits are electronic or nuclear states of individual ions\nIons held in electromagnetic traps, manipulated by lasers\nUsed by: IonQ, Honeywell-Quantinuum\n\n\n\nPhotonic Qubits\n\nQubits are properites of light (e.g., polarization)\nCan operate at room temperature\nUsed by: PsiQuantum, Xanadu\n\n\n\nSemiconductor Quantum Dots\n\nQubits can be encoded using either:\n\ncharge states (electron occupying different quantum dots)\nspin states (up/down spin states of an electron)\n\nCreated by confining electrons in semiconductor nanostructures\nOften called “artificial atoms” due to their discrete energy levels\nActive research at: Intel, TU Delft, Princeton, UNSW, CEA-Leti\n\n\n\nNV Centers in Diamond\n\nThe nitrogen vacancy defects in diamond provide energy levels accessible with light.\nCan operate at room temperature\nApplications in quantum sensing and networking\n\n\n\nTopological Qubits\n\nQubits are non-local and topologically protected states within an exotic (topological) state of matter.\nMost promising candidate: Majorana zero modes\nCurrent Status:\n\nActive research at Microsoft and Delft\nRecent progress in identifying Majorana signatures in nanowires\nDebate continues over experimental evidence\n\n\nAs we’ll discuss, the “perfect” qubit would combine long coherence times, fast gates, high fidelity, easy coupling to other qubits, and straightforward scalability. While each implementation has made significant progress, achieving all these properties simultaneously remains a major challenge in the field.\n\n\n\n\n[1] W. K. Wootters and W. H. Zurek, A single quantum cannot be cloned, Nature 299, 802 (1982).\n\n\n[2] S. Bravyi and A. Kitaev, Universal quantum computation with ideal Clifford gates and noisy ancillas, Physical Review A 71, 022316 (2005).\n\n\n[3] M. M. Wilde, Quantum Information Theory, 2nd ed (Cambridge university press, Cambridge, 2017).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Qubit</span>"
    ]
  },
  {
    "objectID": "qubit.html#footnotes",
    "href": "qubit.html#footnotes",
    "title": "2  The Qubit",
    "section": "",
    "text": "Magic is a technical term in quantum computing, though we’re using it in a colloquial sense here, see  [2].↩︎\nIn some literature, these are matrices are denoted by \\(\\sigma_{x}\\), \\(\\sigma_y\\), and \\(\\sigma_z\\) and related to spin operators via \\(S_i =\\frac12 \\sigma_i\\). This insight can help bridge the idea of these operators and the Bloch sphere.↩︎\nThe divide-by-two for the angles will become clear as we go through this section.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Qubit</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] F.\nArute et al., Quantum supremacy using\na programmable superconducting processor, Nature\n574, 505 (2019).\n\n\n[2] Google Quantum AI and Collaborators et al., Quantum error\ncorrection below the surface code threshold, Nature (2024).\n\n\n[3] E.\nPednault, J. A. Gunnels, G. Nannicini, L. Horesh, and R. Wisnieff, Leveraging\nSecondary Storage to Simulate Deep 54-qubit\nSycamore Circuits, arXiv:1910.09534 (2019).\n\n\n[4] F.\nPan, K. Chen, and P. Zhang, Solving the\nSampling Problem of the Sycamore Quantum\nCircuits, Physical Review Letters 129,\n090502 (2022).\n\n\n[5] M.\nA. Nielsen and I. L. Chuang, Quantum Computation and\nQuantum Information: 10th Anniversary\nEdition, Anniversary edition (Cambridge University Press,\nCambridge ; New York, 2011).\n\n\n[6] A.\nM. Turing, On\nComputable Numbers, with an Application to the\nEntscheidungsproblem, Proceedings of the London\nMathematical Society s2-42, 230 (1937).\n\n\n[7] L.\nHoddeson, The\nDiscovery of the Point-Contact Transistor,\nHistorical Studies in the Physical Sciences 12, 41\n(1981).\n\n\n[8] S.\nBravyi and A. Kitaev, Universal quantum\ncomputation with ideal Clifford gates and noisy\nancillas, Physical Review A 71, 022316\n(2005).\n\n\n[9] R.\nP. Feynman, Simulating\nphysics with computers, International Journal of Theoretical Physics\n21, 467 (1982).\n\n\n[10] D.\nDeutsch, Quantum\ntheory, the Church–Turing principle and the\nuniversal quantum computer, Proceedings of the Royal Society of\nLondon. A. Mathematical and Physical Sciences 400, 97\n(1985).\n\n\n[11] D.\nDeutsch and R. Jozsa, Rapid solution of problems\nby quantum computation, Proceedings of the Royal Society of London.\nSeries A: Mathematical and Physical Sciences 439, 553\n(1992).\n\n\n[12] E.\nBernstein and U. Vazirani, Quantum\nComplexity Theory, SIAM Journal on Computing\n26, 1411 (1997).\n\n\n[13] D.\nR. Simon, On the\nPower of Quantum Computation, SIAM Journal\non Computing 26, 1474 (1997).\n\n\n[14] P.\nW. Shor, Algorithms for Quantum\nComputation: Discrete Logarithms and Factoring, in\nProceedings 35th Annual Symposium on\nFoundations of Computer Science (IEEE\nComput. Soc. Press, Santa Fe, NM, USA, 1994), pp. 124–134.\n\n\n[15] L.\nK. Grover, A Fast\nQuantum Mechanical Algorithm for Database Search, in\nProceedings of the Twenty-Eighth Annual ACM Symposium\non Theory of Computing - STOC ’96 (ACM\nPress, Philadelphia, Pennsylvania, United States, 1996), pp.\n212–219.\n\n\n[16] W.\nK. Wootters and W. H. Zurek, A single quantum cannot be\ncloned, Nature 299, 802 (1982).\n\n\n[17] M.\nM. Wilde, Quantum Information Theory, 2nd ed (Cambridge\nuniversity press, Cambridge, 2017).",
    "crumbs": [
      "References"
    ]
  }
]